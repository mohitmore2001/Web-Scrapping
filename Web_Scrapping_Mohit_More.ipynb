{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b582d143",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11f558a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9cbafb",
   "metadata": {},
   "source": [
    "### URL of the Required Web Scrapping Website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b756d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://clutch.co\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80acd6db",
   "metadata": {},
   "source": [
    "### parseing the html of the main Website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf1bd0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "response34 = requests.get(url)\n",
    "soup34 = bs(response34.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24f678b",
   "metadata": {},
   "source": [
    "### Extracting all the Domains Present in the Website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1dd2111c",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_all = []\n",
    "name_all = []\n",
    "\n",
    "for a in soup34.findAll('a',attrs ={'class':'sitemap-nav__item'}):\n",
    "    \n",
    "    url_all.append(a['href'])\n",
    "    \n",
    "    name_all.append(a.text.replace('\\t',\"\").replace(\"/\",\"_\").replace(\"&\",\"\").replace(\" \",\"_\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e12fae9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MobileApp_Development', 'Software_Development', 'Web_Development', 'AR_VR', 'Artificial_Intelligence', 'Blockchain', 'Web_Design', 'User_Experience_Design', 'Logo_Design', 'Graphic_Design', 'Design', 'Digital_Design', 'Digital_Marketing', 'SEO', 'Social_Media_Marketing', 'Mobile_Marketing', 'Content_Marketing', 'Search_Marketing', 'Advertising', 'Branding', 'Creative', 'Video_Production', 'Public_Relations', 'Media_Planning__Buying', 'Call_Centers', 'BPO', 'Accounting', 'Commercial_Real_Estate', 'HR_Services', 'Consulting', 'IT_Services', 'Cybersecurity', 'Data_Analytics', 'Managed_IT_Services', 'Cloud_Consulting', 'Staff_Augmentation']\n"
     ]
    }
   ],
   "source": [
    "print(name_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdfb957",
   "metadata": {},
   "source": [
    "### Enter the Domain name required. Please copy the name from above to avoid error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33598a05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the Name of the Domain Required from the above Options : Consulting\n"
     ]
    }
   ],
   "source": [
    "name_domain = input(\"Enter the Name of the Domain Required from the above Options : \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3277298",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_no = name_all.index(name_domain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033e2bcf",
   "metadata": {},
   "source": [
    "### Web Scrapping of Information and Converting into csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373db6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "url2 =  url+url_all[index_no]\n",
    "response2 = requests.get(url2)\n",
    "soup2 = bs(response2.content, 'html.parser')\n",
    "    \n",
    "s = soup2.find('li', attrs={'class':'page-item last'})\n",
    "last = s.contents[0]['data-page']\n",
    "    \n",
    "Company = []\n",
    "Website = []\n",
    "Location = []\n",
    "Contact = []\n",
    "Rating = []\n",
    "Review_Count = []\n",
    "Hourly_Rate = []\n",
    "Min_Project_Size = []\n",
    "Employee_Size = []\n",
    "\n",
    "### Loop for iterating all pages for Specific Domain\n",
    "\n",
    "for b in range(0,int(last)+1):\n",
    "    url3 = url2+'?page='+str(b)\n",
    "    response3 = requests.get(url3)\n",
    "    soup3 = bs(response3.content, 'html.parser')\n",
    "        \n",
    "    ### Loop for iterating througth one page for class \"provider provider-row sponsor\" where each class contain information of one company\n",
    "\n",
    "    for a in soup3.findAll(\"li\",attrs={\"class\",\"provider provider-row sponsor\"}):\n",
    "        \n",
    "        ## For Contact\n",
    "        con = a.find('a', attrs={'data-link_text':'Profile Title'})\n",
    "        url4 = 'https://clutch.co'+con['href']+'#summary'\n",
    "        try:\n",
    "            response2 = requests.get(url4)\n",
    "            soup2 = bs(response2.content, 'html.parser')    \n",
    "            cont = soup2.find(\"a\",attrs={\"class\",\"contact phone_icon\"})\n",
    "            Contact.append(cont.text.replace(\" \",\"\").replace(\"\\n\",\"\"))\n",
    "        except:\n",
    "            Contact.append(None)\n",
    "        \n",
    "        ## For Company Name\n",
    "        try:\n",
    "            comp_name = a.find('h3', attrs={'class':'company_info'})\n",
    "            Company.append(comp_name.text.replace(\" \",\"\").replace(\"\\n\",\"\"))\n",
    "        except:\n",
    "            Company.append(None)\n",
    "          \n",
    "        ## For Website Url\n",
    "        try:\n",
    "            web_name = a.find('a', attrs={'class':'website-link__item'})\n",
    "            domain = urlparse(web_name['href']).netloc\n",
    "            Website.append(domain)\n",
    "        except:\n",
    "            Website.append(None)\n",
    "            \n",
    "        ## For Location\n",
    "        try:\n",
    "            loc_name = a.find('div', attrs={'data-content':'<i>Location</i>'})\n",
    "            Location.append(loc_name.text.replace(\"\\n\",\"\"))\n",
    "        except:\n",
    "            Location.append(None)\n",
    "\n",
    "        ## For Rating\n",
    "        try:\n",
    "            rat_name = a.find('span', attrs={'class':'rating sg-rating__number'})\n",
    "            Rating.append(rat_name.text.replace(\" \",\"\").replace(\"\\n\",\"\"))\n",
    "        except:\n",
    "            Rating.append(None)\n",
    "\n",
    "        ## For Review Count\n",
    "        try:\n",
    "            review_name = a.find('a', attrs={'data-link_text':'Reviews Count'})\n",
    "            Review_Count.append(review_name.text.replace(\" \",\"\").replace(\"\\n\",\"\"))\n",
    "        except:\n",
    "            Review_Count.append(None)\n",
    "    \n",
    "        ## For Hourly Rate\n",
    "        try:\n",
    "            hour_name = a.find('div', attrs={'data-content':'<i>Avg. hourly rate</i>'})\n",
    "            Hourly_Rate.append(hour_name.text.replace(\"\\n\",\"\"))\n",
    "        except:\n",
    "            Hourly_Rate.append(None)\n",
    "        \n",
    "        ## For Minimum Project Size\n",
    "        try:    \n",
    "            min_pro = a.find('div', attrs={'data-content':'<i>Min. project size</i>'})\n",
    "            Min_Project_Size.append(min_pro.text.replace(\"\\n\",\"\"))\n",
    "        except:\n",
    "            Min_Project_Size.append(None)\n",
    "\n",
    "        ## For Employee Size\n",
    "        try:\n",
    "            emp_size = a.find('div', attrs={'data-content':'<i>Employees</i>'})\n",
    "            Employee_Size.append(emp_size.text.replace(\"\\n\",\"\"))\n",
    "        except:\n",
    "            Employee_Size.append(None)\n",
    "       \n",
    "    ### Loop for iterating througth one page for class \"provider provider-row\" where each class contain information of one company\n",
    "    \n",
    "    for a in soup3.findAll(\"li\",attrs={\"class\",\"provider provider-row\"}):\n",
    "        \n",
    "        \n",
    "        ## For Contact\n",
    "        con = a.find('a', attrs={'data-link_text':'Profile Title'})\n",
    "        url4 = 'https://clutch.co'+con['href']+'#summary'\n",
    "        try:\n",
    "            response2 = requests.get(url4)\n",
    "            soup2 = bs(response2.content, 'html.parser')    \n",
    "            cont = soup2.find(\"a\",attrs={\"class\",\"contact phone_icon\"})\n",
    "            Contact.append(cont.text.replace(\" \",\"\").replace(\"\\n\",\"\"))\n",
    "        except:\n",
    "            Contact.append(None)\n",
    "        \n",
    "        ## For Company Name\n",
    "        try:\n",
    "            comp_name = a.find('h3', attrs={'class':'company_info'})\n",
    "            Company.append(comp_name.text.replace(\" \",\"\").replace(\"\\n\",\"\"))\n",
    "        except:\n",
    "            Company.append(None)\n",
    "          \n",
    "        ## For Website Url\n",
    "        try:\n",
    "            web_name = a.find('a', attrs={'class':'website-link__item'})\n",
    "            domain = urlparse(web_name['href']).netloc\n",
    "            Website.append(domain)\n",
    "        except:\n",
    "            Website.append(None)\n",
    "            \n",
    "        ## For Location\n",
    "        try:\n",
    "            loc_name = a.find('div', attrs={'data-content':'<i>Location</i>'})\n",
    "            Location.append(loc_name.text.replace(\"\\n\",\"\"))\n",
    "        except:\n",
    "            Location.append(None)\n",
    "\n",
    "        ## For Rating\n",
    "        try:\n",
    "            rat_name = a.find('span', attrs={'class':'rating sg-rating__number'})\n",
    "            Rating.append(rat_name.text.replace(\" \",\"\").replace(\"\\n\",\"\"))\n",
    "        except:\n",
    "            Rating.append(None)\n",
    "\n",
    "        ## For Review Count\n",
    "        try:\n",
    "            review_name = a.find('a', attrs={'data-link_text':'Reviews Count'})\n",
    "            Review_Count.append(review_name.text.replace(\" \",\"\").replace(\"\\n\",\"\"))\n",
    "        except:\n",
    "            Review_Count.append(None)\n",
    "    \n",
    "        ## For Hourly Rate\n",
    "        try:\n",
    "            hour_name = a.find('div', attrs={'data-content':'<i>Avg. hourly rate</i>'})\n",
    "            Hourly_Rate.append(hour_name.text.replace(\"\\n\",\"\"))\n",
    "        except:\n",
    "            Hourly_Rate.append(None)\n",
    "        \n",
    "        ## For Minimum Project Size\n",
    "        try:    \n",
    "            min_pro = a.find('div', attrs={'data-content':'<i>Min. project size</i>'})\n",
    "            Min_Project_Size.append(min_pro.text.replace(\"\\n\",\"\"))\n",
    "        except:\n",
    "            Min_Project_Size.append(None)\n",
    "\n",
    "        ## For Employee Size\n",
    "        try:\n",
    "            emp_size = a.find('div', attrs={'data-content':'<i>Employees</i>'})\n",
    "            Employee_Size.append(emp_size.text.replace(\"\\n\",\"\"))\n",
    "        except:\n",
    "            Employee_Size.append(None)\n",
    "        \n",
    " \n",
    "\n",
    "df = pd.DataFrame({\"Company\":Company,\"Website\":Website,\"Location\":Location,\"Contact\":Contact,\"Rating\":Rating,\"Review Count\":Review_Count,\"Hourly Rate\":Hourly_Rate,\"Min Project Size\":Min_Project_Size,\"Employee Size\":Employee_Size}) \n",
    "df.to_csv(str(name_all[index_no])+'.csv', index=False, encoding='utf-8') \n",
    "print(str(name_all[index_no])+' Scrapping Completed')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ceb145",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
